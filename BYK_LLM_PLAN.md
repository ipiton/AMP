# BYK (Bring Your own Key) LLM Integration Plan

**Date:** 2025-12-02
**Status:** üöß **NEEDS IMPLEMENTATION**

---

## ‚ùå **–ü—Ä–æ–±–ª–µ–º–∞**

–í —Ç–µ–∫—É—â–µ–π OSS –≤–µ—Ä—Å–∏–∏ **—É–¥–∞–ª–µ–Ω –≤–µ—Å—å LLM –∫–æ–¥**, –Ω–æ:
- ‚úÖ BYK (Bring Your own Key) LLM –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å **–±–∞–∑–æ–≤—ã–º OSS —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º**
- ‚ùå –£–¥–∞–ª–∏–ª–∏ –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–π, –∏ OSS-friendly –∫–æ–¥

---

## ‚úÖ **–†–µ—à–µ–Ω–∏–µ: BYK LLM Integration**

### –ß—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ OSS:

#### 1Ô∏è‚É£ **Generic LLM Client** (–ø—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
```go
// –ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—É–±–ª–∏—á–Ω—ã–º–∏ LLM API
- OpenAI API (gpt-4, gpt-3.5-turbo)
- Anthropic Claude API (claude-3-opus, claude-3-sonnet)
- Google Gemini API
- Local LLMs (Ollama, LM Studio)
```

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ ENV:**
```bash
LLM_ENABLED=true
LLM_PROVIDER=openai         # openai, anthropic, google, ollama
LLM_API_KEY=sk-...          # User's API key (BYK!)
LLM_MODEL=gpt-4o
LLM_BASE_URL=https://api.openai.com/v1  # Optional override
```

#### 2Ô∏è‚É£ **Classification Service**
```go
// Classification —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ fallback
- L1 cache (in-memory)
- L2 cache (Redis)
- Intelligent fallback (rule-based)
- Batch processing
- Circuit breaker
```

#### 3Ô∏è‚É£ **Enrichment Service**
```go
// Enrichment modes
- transparent: No AI (default)
- enriched: Add AI classification
- transparent_with_recommendations: Show AI suggestions
```

#### 4Ô∏è‚É£ **Extension Example**
```go
// examples/custom-llm-classifier/
- –ü—Ä–∏–º–µ—Ä –∫–∞—Å—Ç–æ–º–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ pkg/core interfaces
```

---

## ‚ùå **–ß—Ç–æ –ù–ï –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ OSS:**

1. ‚ùå **–ü—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã** (–µ—Å–ª–∏ –µ—Å—Ç—å —Å–µ–∫—Ä–µ—Ç–Ω—ã–µ)
2. ‚ùå **–ü–ª–∞—Ç–Ω—ã–π LLM –ø—Ä–æ–∫—Å–∏** (llm-proxy.b2broker.tech)
3. ‚ùå **Enterprise-only –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã** (–µ—Å–ª–∏ –µ—Å—Ç—å —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ)
4. ‚ùå **Paid features** (advanced tuning, custom models)

---

## üìã **Implementation Plan**

### Phase 1: Core LLM Client (2-3 hours)
```
‚úÖ –°–æ–∑–¥–∞—Ç—å pkg/llm/
‚îú‚îÄ‚îÄ client.go         - LLMClient interface
‚îú‚îÄ‚îÄ openai.go         - OpenAI implementation
‚îú‚îÄ‚îÄ anthropic.go      - Anthropic implementation
‚îú‚îÄ‚îÄ local.go          - Local LLM (Ollama)
‚îî‚îÄ‚îÄ errors.go         - Error types
```

**Features:**
- Direct API integration (no proxy)
- Standard OpenAI/Anthropic SDK
- Retry with exponential backoff
- Context timeout support
- Streaming support (optional)

### Phase 2: Classification Service (1-2 hours)
```
‚úÖ –°–æ–∑–¥–∞—Ç—å internal/core/services/
‚îú‚îÄ‚îÄ classification.go         - ClassificationService interface
‚îú‚îÄ‚îÄ classification_impl.go    - Implementation
‚îú‚îÄ‚îÄ classification_cache.go   - Two-tier caching
‚îú‚îÄ‚îÄ classification_fallback.go - Rule-based fallback
‚îî‚îÄ‚îÄ classification_test.go    - Tests
```

**Features:**
- Two-tier caching (L1 memory + L2 Redis)
- Circuit breaker (via resilience package)
- Intelligent fallback
- Batch processing
- Prometheus metrics

### Phase 3: Enrichment Service (1 hour)
```
‚úÖ –°–æ–∑–¥–∞—Ç—å internal/core/services/
‚îú‚îÄ‚îÄ enrichment.go       - EnrichmentService interface
‚îú‚îÄ‚îÄ enrichment_impl.go  - Implementation
‚îú‚îÄ‚îÄ enrichment_modes.go - transparent/enriched/recommendations
‚îî‚îÄ‚îÄ enrichment_test.go  - Tests
```

**Features:**
- Mode toggle (Redis-backed)
- Graceful degradation
- Performance tracking

### Phase 4: Integration (1 hour)
```
‚úÖ –û–±–Ω–æ–≤–∏—Ç—å main.go
- Optional LLM initialization (if LLM_ENABLED=true)
- Classification service registration
- Enrichment mode manager
- Alert processor integration
```

### Phase 5: Documentation (1 hour)
```
‚úÖ –°–æ–∑–¥–∞—Ç—å docs/
‚îú‚îÄ‚îÄ BYK_LLM_GUIDE.md           - User guide
‚îú‚îÄ‚îÄ LLM_PROVIDERS.md           - Provider comparison
‚îî‚îÄ‚îÄ MIGRATION_FROM_PROXY.md    - Migration from proprietary proxy
```

### Phase 6: Examples (1 hour)
```
‚úÖ –°–æ–∑–¥–∞—Ç—å examples/
‚îî‚îÄ‚îÄ custom-llm-classifier/
    ‚îú‚îÄ‚îÄ main.go              - Custom LLM integration example
    ‚îú‚îÄ‚îÄ provider.go          - Custom provider
    ‚îî‚îÄ‚îÄ README.md            - Documentation
```

---

## üéØ **Expected Timeline**

| Phase | Duration | Priority |
|-------|----------|----------|
| Phase 1: Core LLM Client | 2-3h | P0 (Critical) |
| Phase 2: Classification Service | 1-2h | P0 (Critical) |
| Phase 3: Enrichment Service | 1h | P1 (High) |
| Phase 4: Integration | 1h | P0 (Critical) |
| Phase 5: Documentation | 1h | P1 (High) |
| Phase 6: Examples | 1h | P2 (Medium) |

**Total:** 7-9 hours

---

## üìä **Benefits**

### For Users:
- ‚úÖ **Free AI classification** (using their own API keys)
- ‚úÖ **Choice of provider** (OpenAI, Anthropic, Google, Local)
- ‚úÖ **No vendor lock-in** (standard APIs)
- ‚úÖ **Privacy-friendly** (no third-party proxy)
- ‚úÖ **Cost control** (their own billing)

### For Project:
- ‚úÖ **Competitive feature** (vs Alertmanager)
- ‚úÖ **Community adoption** (AI is trendy)
- ‚úÖ **Extension point** (custom classifiers)
- ‚úÖ **100% OSS** (no proprietary code)

---

## üîß **Technical Requirements**

### Dependencies:
```go
// OpenAI SDK
"github.com/sashabaranov/go-openai" v1.20.0

// Anthropic SDK (community)
"github.com/liushuangls/go-anthropic" v0.5.0

// Ollama SDK
"github.com/ollama/ollama/api" latest
```

### Configuration:
```yaml
llm:
  enabled: true                          # Default: false
  provider: openai                       # openai, anthropic, google, ollama
  api_key: ${LLM_API_KEY}               # Required if enabled
  model: gpt-4o                         # Provider-specific
  base_url: https://api.openai.com/v1   # Optional override
  timeout: 30s
  max_retries: 3
  enable_cache: true
  cache_ttl: 24h
  enable_fallback: true
```

---

## ‚úÖ **Acceptance Criteria**

### Must Have (MVP):
- [ ] OpenAI integration working
- [ ] Classification service with caching
- [ ] Enrichment modes (transparent/enriched)
- [ ] Alert processor integration
- [ ] Basic documentation
- [ ] Configuration via ENV

### Nice to Have (Post-MVP):
- [ ] Anthropic integration
- [ ] Google Gemini integration
- [ ] Local LLM support (Ollama)
- [ ] Streaming support
- [ ] Custom prompt templates
- [ ] Fine-tuning support

---

## üöÄ **Next Steps**

1. **Review this plan** with team
2. **Start Phase 1** (Core LLM Client)
3. **Update ROADMAP.md** (add BYK LLM to v1.1.0)
4. **Create GitHub issue** (track progress)
5. **Communicate to community** (feature announcement)

---

## üìö **References**

- OpenAI API: https://platform.openai.com/docs/api-reference
- Anthropic API: https://docs.anthropic.com/claude/reference
- Ollama: https://ollama.ai/
- BYK Pattern: https://en.wikipedia.org/wiki/Bring_your_own_key

---

**Status:** READY FOR IMPLEMENTATION
**Priority:** P0 (Should be in v1.0.0 or v1.1.0)
**Estimated Effort:** 7-9 hours
